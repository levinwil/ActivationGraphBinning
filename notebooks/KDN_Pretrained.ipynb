{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPaqPKs4ZvVa"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 6\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cha_h-Smq2z1"
   },
   "outputs": [],
   "source": [
    "num_trials = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzuhRa_v8IrC"
   },
   "source": [
    "# Construct & Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aA3RJ4NZ8LqF",
    "outputId": "cc729b13-b420-4dc7-e851-e68aa2e93982"
   },
   "outputs": [],
   "source": [
    "def construct_network(X, y):\n",
    "    base_network = keras.applications.resnet50.ResNet50(input_shape = np.shape(X)[1:], include_top = False, pooling = 'max')\n",
    "    for layers in base_network.layers:\n",
    "        layers.trainable = False\n",
    "        \n",
    "    network = keras.Sequential()\n",
    "    network.add(base_network)\n",
    "    network.add(keras.layers.Dense(256, activation = 'relu'))\n",
    "    network.add(keras.layers.Dense(256, activation = 'relu'))\n",
    "    network.add(keras.layers.Dense(units=len(np.unique(y)), activation = 'softmax'))\n",
    "    network.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(3e-4))\n",
    "    network.fit(\n",
    "      X, \n",
    "      keras.utils.to_categorical(y), \n",
    "      epochs = 10, \n",
    "      verbose = True,\n",
    "      batch_size = int(1.7 ** (np.log(len(X)) / np.log(5) + 1.5))\n",
    "    )\n",
    "    return network\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keurnal Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AKN(X, y):\n",
    "    X_transform, X, y_transform, y = train_test_split(X, y, test_size = 0.1)\n",
    "    #X_transform, y_transform = X, y\n",
    "    network = construct_network(X_transform, y_transform)\n",
    "    \n",
    "    encoder = keras.models.Model(network.inputs, network.layers[-3].input)\n",
    "    X = encoder.predict(X)\n",
    "    polytope_memberships = []\n",
    "    last_activations = X\n",
    "    print(np.shape(last_activations))\n",
    "    for layer_id in range(len(network.layers) - 3, len(network.layers)):\n",
    "        weights, bias = network.layers[layer_id].get_weights()\n",
    "        print(np.shape(weights))\n",
    "        preactivation = np.matmul(last_activations, weights) + bias\n",
    "        if layer_id == len(network.layers) - 1:\n",
    "            binary_preactivation = (preactivation > 0.5).astype('int')\n",
    "        else:\n",
    "            binary_preactivation = (preactivation > 0).astype('int')\n",
    "        polytope_memberships.append(binary_preactivation)\n",
    "        last_activations = preactivation * binary_preactivation\n",
    "\n",
    "    polytope_memberships = np.concatenate(polytope_memberships, axis = 1)\n",
    "    _, polytope_ids = np.unique(np.matmul(polytope_memberships, 2 ** np.arange(0, np.shape(polytope_memberships)[1])), return_inverse = True)\n",
    "\n",
    "\n",
    "    #an array of all of the X-values of the class-wise means in each leaf\n",
    "    polytope_means_X = []\n",
    "\n",
    "    #an array of all of the y-values (i.e. class values) of the class-wise means in each leaf\n",
    "    polytope_means_y = []\n",
    "\n",
    "    #an array of all of the number of points that comprise the means in each leaf\n",
    "    polytope_means_weight = []\n",
    "\n",
    "    #an array of all of the average variances of the points in each leaf corresponding to \n",
    "    #a single class from the class-wise mean in that leaf\n",
    "    polytope_means_var = []\n",
    "\n",
    "    polytope_KDEs = []\n",
    "\n",
    "    for polytope_value in np.unique(polytope_ids):\n",
    "        for y_val in np.unique(y):\n",
    "            idxs_in_polytope_of_class = np.where((y == y_val) & (polytope_ids == polytope_value))[0]\n",
    "            if len(idxs_in_polytope_of_class) > 1:\n",
    "                mean_X_in_polytope_of_class = np.mean(X[idxs_in_polytope_of_class], axis = 0)\n",
    "                polytope_means_X.append(mean_X_in_polytope_of_class)\n",
    "                #we already know the y value, so just append it \n",
    "                polytope_means_y.append(y_val)\n",
    "                #compute the number of points in that leaf corresponding to that y value\n",
    "                #and append to the aggregate array\n",
    "                polytope_means_weight.append(len(idxs_in_polytope_of_class))\n",
    "                #compute the distances of all the points in that leaf corresponding to that y value\n",
    "                #from the mean X-value of the points in that leaf corresponding to that y value\n",
    "                dists_in_polytope = np.sqrt(np.sum((X[idxs_in_polytope_of_class] - mean_X_in_polytope_of_class)**2, axis = 1))\n",
    "                #compute the variance as the average distance of the class-wise points in that leaf \n",
    "                #and append to the aggregate array\n",
    "                polytope_means_var.append(np.mean(dists_in_polytope))\n",
    "\n",
    "\n",
    "\n",
    "    #convert to numpy array so we can easily refer to multiple sets of points as ra[idxs]\n",
    "    polytope_means_y, polytope_means_X, polytope_means_weight, polytope_means_var = np.array(polytope_means_y), np.array(polytope_means_X), np.array(polytope_means_weight), np.array(polytope_means_var)\n",
    "\n",
    "    #an array of all of the KDEs. Each KDE will be respondsible for computing the probability \n",
    "    #that a given set of inference points belongs to that class. Thus, we have separate KDEs \n",
    "    #for each class. The KDE at index i is the KDE that is responsible for computations on \n",
    "    #y_value = i\n",
    "    KDEs = []\n",
    "    #loop over the y values in the leaf means \n",
    "    for y_val in np.unique(polytope_means_y):\n",
    "        #gather all of the leaf means corresponding to the given y value\n",
    "        polytope_means_X_of_y_val = polytope_means_X[np.where(polytope_means_y == y_val)[0]]\n",
    "        #father all of the weights corresponding to the given y value\n",
    "        polytope_means_weight_of_y_val = polytope_means_weight[np.where(polytope_means_y == y_val)[0]]\n",
    "        #compute the bandwidth as the average variance across the class-wise leaf means, weighted\n",
    "        #by the number of points\n",
    "        polytope_means_var_of_y_val = polytope_means_var[np.where(polytope_means_y == y_val)[0]]\n",
    "        bandwidth=np.average(polytope_means_var_of_y_val, weights = polytope_means_weight_of_y_val)\n",
    "        #train an sklearn KDE with a gaussian kernel with the bandwidth calculated above\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth)\n",
    "        #fit the KDE on the leaf means corresponding to the given y value, weighted by\n",
    "        #the weights corresponding to the given y value\n",
    "        kde.fit(X = polytope_means_X_of_y_val, sample_weight = polytope_means_weight_of_y_val)\n",
    "\n",
    "        #append the KDE to the aggregate array\n",
    "        KDEs.append(kde)\n",
    "    return KDEs, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_mean(ra, low = 25, high = 75):\n",
    "    ra = np.array(ra)\n",
    "    lower_val = np.nanpercentile(ra, low)\n",
    "    higher_val = np.nanpercentile(ra, high)\n",
    "    return np.mean(ra[np.where((ra >= lower_val) & (ra <= higher_val))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ece(predicted_posterior, y):\n",
    "    hists = []\n",
    "    hists_hat = []\n",
    "    amts = []\n",
    "    num_bins = 40\n",
    "    eces_across_y_vals = []\n",
    "    for y_val in np.unique(y):\n",
    "        for i in range(num_bins):\n",
    "            prop = i*1./num_bins\n",
    "            inds = np.where((predicted_posterior[:, y_val] >= prop) & (predicted_posterior[:, y_val] <= prop+1./num_bins))[0]\n",
    "            amts.append(len(inds))\n",
    "            if len(inds) > 0:\n",
    "                hists.append(len(np.where(y[inds] == y_val)[0])*1./len(inds))\n",
    "                hists_hat.append(np.mean(predicted_posterior[inds, y_val]))\n",
    "            else:\n",
    "                hists.append(prop)\n",
    "                hists_hat.append(prop + 0.5/num_bins)\n",
    "        eces_across_y_vals.append(np.dot(np.abs(np.array(hists) - np.array(hists_hat)), amts) / np.sum(amts))\n",
    "        return np.mean(eces_across_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brier(predicted_posterior, y):\n",
    "    brier_across_y_vals = []\n",
    "    for y_val in np.unique(y):\n",
    "        brier_across_y_vals.append(np.nanmean((predicted_posterior[:, y_val] - (y == y_val).astype('int'))**2))\n",
    "    return np.mean(brier_across_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(X, y), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\\ntrain_indices, test_indices = np.where(y < 2)[0], np.where(y_test < 2)[0]\\nX, y = np.expand_dims(X[train_indices], axis = -1), y[train_indices]\\nX_test, y_test = np.expand_dims(X_test[test_indices], axis = -1), y_test[test_indices]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(X, y), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "train_indices, test_indices = np.where(y < 2)[0], np.where(y_test < 2)[0]\n",
    "X, y = np.expand_dims(X[train_indices], axis = -1), y[train_indices]\n",
    "X_test, y_test = np.expand_dims(X_test[test_indices], axis = -1), y_test[test_indices]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "train_indices, test_indices = np.where(y < 2)[0], np.where(y_test < 2)[0]\n",
    "X, y = X[train_indices], y[train_indices, 0]\n",
    "X_test, y_test = X_test[test_indices], y_test[test_indices, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KDE_y_proba(X, y, n, X_test):\n",
    "    random_indices = np.random.choice(len(X), int(n))\n",
    "    KDEs, encoder = train_AKN(X, y)\n",
    "    X, y = encoder.predict(X[random_indices]), y[random_indices]\n",
    "    X_test = encoder.predict(X_test)\n",
    "    y_proba_unnormalized = np.zeros((len(X_test), len(KDEs)))\n",
    "    #loops over the KDEs (and thus, implicitly, the y-values)\n",
    "    for y_val in range(len(KDEs)):\n",
    "        #compute the (unnormalized) posterior for the KDE corresponding\n",
    "        #to y_val. NOTE: we perform np.exp since the score_samples \n",
    "        #function returns the log of the probability of belonging to that \n",
    "        #class, so we must invert the log through an exponential\n",
    "        y_proba_unnormalized[:, y_val] = np.exp(KDEs[y_val].score_samples(X_test))\n",
    "\n",
    "    y_proba = y_proba_unnormalized.copy()\n",
    "    for y_val in range(np.shape(y_proba)[1]):\n",
    "        y_proba[:, y_val] /= np.sum(y_proba_unnormalized, axis = 1)\n",
    "    \n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf175dae5184accb50e35954398bba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b1dc859e4b4c68987cc6f0b991c9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 5s 526us/sample - loss: 0.4058\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 2s 231us/sample - loss: 0.3175\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 2s 235us/sample - loss: 0.2802\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 2s 235us/sample - loss: 0.2536\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 2s 240us/sample - loss: 0.2414\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 2s 235us/sample - loss: 0.2142\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 2s 233us/sample - loss: 0.2081\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 2s 234us/sample - loss: 0.1949\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 2s 235us/sample - loss: 0.1815\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 2s 238us/sample - loss: 0.1780\n",
      "(1000, 2048)\n",
      "(2048, 256)\n",
      "(256, 256)\n",
      "(256, 2)\n",
      "Train on 9000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 5s 565us/sample - loss: 0.4319\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 2s 239us/sample - loss: 0.3144\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 2s 237us/sample - loss: 0.2839\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 2s 239us/sample - loss: 0.2637\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 2s 243us/sample - loss: 0.2308\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 2s 239us/sample - loss: 0.2144\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 2s 238us/sample - loss: 0.2049\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 2s 239us/sample - loss: 0.1936\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 2s 242us/sample - loss: 0.1841\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 2s 252us/sample - loss: 0.1682\n",
      "(1000, 2048)\n",
      "(2048, 256)\n",
      "(256, 256)\n",
      "(256, 2)\n",
      "Train on 9000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 5s 529us/sample - loss: 0.4240\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 2s 240us/sample - loss: 0.3193\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 2s 236us/sample - loss: 0.2707\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 2s 246us/sample - loss: 0.2544\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 2s 241us/sample - loss: 0.2351\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 2s 232us/sample - loss: 0.2169\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 2s 233us/sample - loss: 0.2036\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 2s 234us/sample - loss: 0.1970\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 2s 234us/sample - loss: 0.1855\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 2s 239us/sample - loss: 0.1818\n",
      "(1000, 2048)\n",
      "(2048, 256)\n",
      "(256, 256)\n",
      "(256, 2)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-31a0fd71629c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mKDE_briers_across_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mKDE_accs_across_trials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKDE_y_test_proba_across_trials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrial_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mKDE_briers_across_trials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKDE_y_test_proba_across_trials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrial_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "KDE_acc_means = []\n",
    "\n",
    "KDE_acc_stds = []\n",
    "\n",
    "KDE_brier_means = []\n",
    "\n",
    "KDE_brier_stds = []\n",
    "n_ra = np.logspace(5.5, 8.5, num = 10, base = 10)\n",
    "ticks = np.arange(np.min(n_ra), np.max(n_ra), step = int((np.max(n_ra) - np.min(n_ra)) // 4))\n",
    "ticks_ra = np.array([int(str(tick)[:1]) * 10 ** int(np.log10(tick)) for tick in ticks])\n",
    "for n in tqdm(n_ra):\n",
    "    KDE_y_test_proba_across_trials = np.array([get_KDE_y_proba(X, y, n, X_test) for _ in tqdm(range(num_trials))])\n",
    "\n",
    "    \n",
    "    KDE_accs_across_trials = []\n",
    "    KDE_briers_across_trials = []\n",
    "    for trial_idx in range(num_trials):\n",
    "        KDE_accs_across_trials.append(np.nanmean(np.argmax(KDE_y_test_proba_across_trials[trial_idx], axis = 1) == y_test))\n",
    "        \n",
    "        KDE_briers_across_trials.append(get_ece(KDE_y_test_proba_across_trials[trial_idx], y_test))\n",
    "        \n",
    "    KDE_acc_means.append(clipped_mean(KDE_accs_across_trials, 25, 75))\n",
    "    \n",
    "    KDE_brier_means.append(clipped_mean(KDE_briers_across_trials, 0, 50))\n",
    "\n",
    "    figs, ax = plt.subplots(1, 2, figsize = (18, 9))\n",
    "    figs.set_facecolor(\"white\")\n",
    "\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=27)\n",
    "    ax[0].plot(n_ra[:len(KDE_acc_means)] , KDE_acc_means, label = \"Ours\", c = \"red\")\n",
    "    ax[0].hlines(1.0, 0, n_ra[len(KDE_acc_means) - 1], linestyle = \"dashed\", label = \"Perfect Class Prediction\", color = \"black\")\n",
    "    ax[0].legend(fontsize = 18)\n",
    "    ax[0].set_xlabel(\"Number of Training Samples (logscale)\", fontsize = 27)\n",
    "    ax[0].set_ylabel(\"Test Accuracy\", fontsize = 27)\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].set_xticks(10 ** np.arange(np.log10(np.min(ticks_ra[:len(KDE_acc_means)])), np.log10(np.max(ticks_ra[:len(KDE_acc_means)]))))\n",
    "\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=27)\n",
    "    ax[1].plot(n_ra[:len(KDE_acc_means)] , KDE_brier_means, label = \"Ours\", c = \"red\")\n",
    "    ax[1].hlines(0.0, 0, n_ra[len(KDE_acc_means) - 1], linestyle = \"dashed\", label = \"Perfect Calibration\", color = \"black\")\n",
    "    ax[1].legend(fontsize = 18)\n",
    "    ax[1].set_xlabel(\"Number of Training Samples (logscale)\", fontsize = 27)\n",
    "    ax[1].set_ylabel(\"Test mECE\", fontsize = 27)\n",
    "    ax[1].set_xscale(\"log\")\n",
    "    ax[1].set_xticks(10 ** np.arange(np.log10(np.min(ticks_ra[:len(KDE_acc_means)])), np.log10(np.max(ticks_ra[:len(KDE_acc_means)]))))\n",
    "\n",
    "    figs.tight_layout()\n",
    "\n",
    "    figs.suptitle(\"Posterior Estimation Comparison\\nDataset: Gaussian XOR w/ Sigma=0.1\", fontsize=27, y = 1.15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, ax = plt.subplots(1, 2, figsize = (18, 9))\n",
    "figs.set_facecolor(\"white\")\n",
    "\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=27)\n",
    "ax[0].plot(n_ra , KDE_acc_means, label = \"Ours\", c = \"red\")\n",
    "ax[0].plot(n_ra, network_acc_means, label = \"Deep Network\", c = \"blue\")\n",
    "ax[0].hlines(1.0, 0, n_ra[-1], linestyle = \"dashed\", label = \"Perfect Class Prediction\", color = \"black\")\n",
    "ax[0].legend(fontsize = 18)\n",
    "ax[0].set_xlabel(\"Number of Training Samples (logscale)\", fontsize = 27)\n",
    "ax[0].set_ylabel(\"Test Accuracy\", fontsize = 27)\n",
    "ax[0].set_xscale(\"log\")\n",
    "\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=27)\n",
    "ax[1].plot(n_ra , KDE_brier_means, label = \"Ours\", c = \"red\")\n",
    "ax[1].plot(n_ra, network_brier_means, label = \"Deep Network\", c = \"blue\")\n",
    "ax[1].hlines(0.0, 0, n_ra[-1], linestyle = \"dashed\", label = \"Perfect Calibration\", color = \"black\")\n",
    "ax[1].legend(fontsize = 18)\n",
    "ax[1].set_xlabel(\"Number of Training Samples (logscale)\", fontsize = 27)\n",
    "ax[1].set_ylabel(\"Test mECE\", fontsize = 27)\n",
    "ax[1].set_xscale(\"log\")\n",
    "\n",
    "\n",
    "figs.tight_layout()\n",
    "\n",
    "figs.suptitle(\"Posterior Estimation Comparison\\nDataset: Gaussian XOR w/ Sigma=0.1\\nNetwork Widths={}\".format(widths), fontsize=27, y = 1.15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Polytopes",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
